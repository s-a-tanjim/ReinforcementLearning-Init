{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841f7a25-fa49-4190-b6d1-dd12c1a1f243",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc1bed2-2236-4ff0-9d5d-0c23f9efbcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02c02d-a422-431d-ae54-e47a68c2714d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5c9b1b-631b-4a28-ab13-07ccda46f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = None # Path of the model or none\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 10 # 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = '2x256'\n",
    "MAX_TRY_IN_EPISODE = 100 # 200\n",
    "MIN_REWARD = -MAX_TRY_IN_EPISODE # 0 For model save\n",
    "# MEMORY_FRACTION = 0.20 # For GPU\n",
    "\n",
    "# Environment settings\n",
    "EPISODES = 5000 # 20_000\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.9 # 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 50  # episodes\n",
    "\n",
    "SHOW_PREVIEW = True\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce5cf2-7b21-4546-a016-c977ae700c7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c13473-a33c-4f3d-a416-9394c8c96ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blob:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, size)\n",
    "        self.y = np.random.randint(0, size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Blob ({self.x}, {self.y})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choice):\n",
    "        '''\n",
    "        Gives us 4 total movement options. (0,1,2,3)\n",
    "        '''\n",
    "        if choice == 0:\n",
    "            self.move(x=1, y=0)\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1, y=0)\n",
    "        elif choice == 2:\n",
    "            self.move(x=0, y=1)\n",
    "        elif choice == 3:\n",
    "            self.move(x=0, y=-1)\n",
    "\n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "        # If no value for x, move randomly\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "\n",
    "        # If no value for y, move randomly\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > self.size-1:\n",
    "            self.x = self.size-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > self.size-1:\n",
    "            self.y = self.size-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643e9ae-2af3-4d0b-b8c4-eeae9a6e89bc",
   "metadata": {},
   "source": [
    "# Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b00ba8c-03db-4797-838e-a59dcec89fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    SIZE = 10\n",
    "    RESIZE_IMG_SIZE = 50\n",
    "    RETURN_IMAGES = True\n",
    "    MOVE_PENALTY = 1\n",
    "    ENEMY_PENALTY = 300\n",
    "    FOOD_REWARD = 25\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4\n",
    "    ACTION_SPACE_SIZE = 4\n",
    "    PLAYER_N = 1  # player key in dict\n",
    "    FOOD_N = 2  # food key in dict\n",
    "    ENEMY_N = 3  # enemy key in dict\n",
    "    # the dict! (colors)\n",
    "    d = {1: (255, 175, 0),\n",
    "         2: (0, 255, 0),\n",
    "         3: (0, 0, 255)}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GameEnv, self).__init__()\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.action_space = spaces.Discrete(self.ACTION_SPACE_SIZE)\n",
    "        # Example for using image as input:\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "                                            shape=(self.RESIZE_IMG_SIZE, self.RESIZE_IMG_SIZE, 3), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        self.player = Blob(self.SIZE)\n",
    "        self.food = Blob(self.SIZE)\n",
    "        while self.food == self.player:\n",
    "            self.food = Blob(self.SIZE)\n",
    "        self.enemy = Blob(self.SIZE)\n",
    "        while self.enemy == self.player or self.enemy == self.food:\n",
    "            self.enemy = Blob(self.SIZE)\n",
    "\n",
    "        self.episode_step = 0\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            observation = np.array(self.get_image())\n",
    "        else:\n",
    "            observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action(action)\n",
    "        print(\"Steppp\")\n",
    "\n",
    "        #### MAYBE ###\n",
    "        #enemy.move()\n",
    "        #food.move()\n",
    "        ##############\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        else:\n",
    "            new_observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "\n",
    "        if self.player == self.enemy:\n",
    "            reward = -self.ENEMY_PENALTY\n",
    "        elif self.player == self.food:\n",
    "            reward = self.FOOD_REWARD\n",
    "        else:\n",
    "            reward = -self.MOVE_PENALTY\n",
    "\n",
    "        done = False\n",
    "        if reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or self.episode_step >= MAX_TRY_IN_EPISODE:\n",
    "            done = True\n",
    "            \n",
    "        self.render()\n",
    "\n",
    "        return new_observation, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.\n",
    "        cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # FOR CNN #\n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
    "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]  # sets the food location tile to green color\n",
    "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]  # sets the enemy location to red\n",
    "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]  # sets the player tile to blue\n",
    "        img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
    "        img = img.resize((self.RESIZE_IMG_SIZE, self.RESIZE_IMG_SIZE))\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def close(self):\n",
    "        self.__del__()\n",
    "    \n",
    "    def __del__(self):\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e47ce-6111-4d16-adf5-933cacb4bb2c",
   "metadata": {},
   "source": [
    "## Check the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4285bf8d-d928-45d6-bf0e-be30ecd13b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# env = GameEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "# check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec2139-b91b-4987-bc91-71cfcedb0fbb",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59164b33-2ce1-4699-b836-3259b9e5a302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 13:38:02.127612: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models_dir = f\"models/dqn/{int(time.time())}/\"\n",
    "logdir = f\"logs/dqn/{int(time.time())}/\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "\tos.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "\tos.makedirs(logdir)\n",
    "\n",
    "env = GameEnv()\n",
    "env.reset()\n",
    "model = DQN('MlpPolicy', env=env, buffer_size = REPLAY_MEMORY_SIZE, learning_starts = MIN_REPLAY_MEMORY_SIZE, verbose=1, target_update_interval = 50, tensorboard_log=logdir)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "MAX_ITERS = 20 # 2000\n",
    "\n",
    "iters = 0\n",
    "while iters < MAX_ITERS:\n",
    "\ttry:\n",
    "\t\titers += 1\n",
    "\t\tprint(\"Here\")\n",
    "\t\tmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"dqn\")\n",
    "\t\tmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n",
    "\t\n",
    "\texcept KeyboardInterrupt:\n",
    "\t\tenv.close()\n",
    "\t\tprint(\"Exit!\")\n",
    "\t\tbreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
